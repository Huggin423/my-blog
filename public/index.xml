<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>我的博客</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on 我的博客</description>
    <generator>Hugo -- 0.150.0</generator>
    <language>zh-cn</language>
    <lastBuildDate>Wed, 22 Oct 2025 14:32:14 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LoRA_1</title>
      <link>http://localhost:1313/posts/lora/concept/</link>
      <pubDate>Wed, 22 Oct 2025 14:32:14 +0800</pubDate>
      <guid>http://localhost:1313/posts/lora/concept/</guid>
      <description>&lt;h3 id=&#34;utils-for-lora&#34;&gt;Utils for LoRA&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;merge_and_unload(): 将适配器权重与基础模型合并，从而将新合并的模型作为一个独立模型有效使用。&lt;/li&gt;
&lt;li&gt;merge_adapter(): 将LoRA层（与前面的适配器权重的关系？）合并到基础模型中，并保留PEFTModel。&lt;/li&gt;
&lt;li&gt;unmerge_adapter(): 将LoRA层从基础模型解合并，同时保留PEFTModel。&lt;/li&gt;
&lt;li&gt;unload(): 获取未合并LoRA层前的基础模型，主要是为了获得origin model，听说在Stable Diffusion WebUi中会使用（这是什么？）。&lt;/li&gt;
&lt;li&gt;delete_adapter(): 删除现有适配器。&lt;/li&gt;
&lt;li&gt;add_weighted_adapter():根据用户提供的权重方案将多个LoRAs组合成一个新的适配器。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;peft中常见的lora参数&#34;&gt;PEFT中常见的LoRA参数&lt;/h3&gt;
&lt;p&gt;常见的使用LoRA微调模型的流程：
实例化基础模型 &amp;ndash;&amp;gt; 创建配置(LoraConfig)，包含LoRA特定参数 &amp;ndash;&amp;gt; 将基础模型使用get_peft_model包裹，获取可训练的PEFTModel &amp;ndash;&amp;gt; 开始训练&lt;/p&gt;
&lt;p&gt;这里具体介绍LoraConfig包含哪些参数来控制LoRA应用于基础模型。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;rank 更新矩阵的秩，用 int 表示。较低的秩会导致更新矩阵更小，训练参数更少。&lt;/li&gt;
&lt;li&gt;target_modules 应用于 LoRA 更新矩阵的模块&lt;/li&gt;
&lt;li&gt;lora_alpha LoRA 缩放因子&lt;/li&gt;
&lt;li&gt;bias 指定 bias 参数是否需要训练。可以是 &amp;rsquo;none&amp;rsquo; 、 &amp;lsquo;all&amp;rsquo; 或 &amp;rsquo;lora_only&amp;rsquo;&lt;/li&gt;
&lt;li&gt;use_rslora 当设置为 True 时，使用 &lt;code&gt;Rank-Stabilized LoRA&lt;/code&gt;，将适配器缩放因子设置为 &lt;code&gt;lora_alpha/math.sqrt(r)&lt;/code&gt; ，因为它被证明效果更好。否则，它将使用原始默认值 &lt;code&gt;lora_alpha/r&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;modules_to_save 除了 LoRA 层之外，需要设置为可训练并保存在最终检查点中的模块列表。这些通常包括模型的自定义头部，该头部在微调任务中随机初始化。&lt;/li&gt;
&lt;li&gt;layers_to_transform 需要由 LoRA 转换的层列表。如果未指定，则 &lt;code&gt;target_modules&lt;/code&gt; 中的所有层都将被转换。&lt;/li&gt;
&lt;li&gt;layers_pattern 匹配 &lt;code&gt;target_modules&lt;/code&gt; 中层名称的模式，如果指定了 &lt;code&gt;layers_to_transform&lt;/code&gt; 。默认情况下 PeftModel 会查看常见层模式（ layers , h , blocks 等），用于异类和自定义模型。&lt;/li&gt;
&lt;li&gt;rank_pattern 将层名称或正则表达式映射到与 r 指定的默认秩不同的秩。&lt;/li&gt;
&lt;li&gt;alpha_pattern : 将层名称或正则表达式映射到与 &lt;code&gt;lora_alpha&lt;/code&gt;指定的默认 &lt;code&gt;alpha&lt;/code&gt; 不同的 &lt;code&gt;alpha&lt;/code&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;初始化选项&#34;&gt;初始化选项&lt;/h3&gt;
&lt;p&gt;LoRA 权重的初始化由 &lt;code&gt;LoraConfig&lt;/code&gt; 的 &lt;code&gt;init_lora_weights&lt;/code&gt; 参数控制。默认情况下，PEFT 以参考实现相同的方式初始化 LoRA 权重，即使用 &lt;code&gt;Kaiming-uniform&lt;/code&gt; 初始化权重 A，并将权重 B 初始化为零，从而得到恒等变换。&lt;/p&gt;</description>
    </item>
    <item>
      <title>LoRA 实操</title>
      <link>http://localhost:1313/posts/lora/operation/</link>
      <pubDate>Wed, 22 Oct 2025 11:26:01 +0800</pubDate>
      <guid>http://localhost:1313/posts/lora/operation/</guid>
      <description></description>
    </item>
    <item>
      <title>LoRA 总结与衍生</title>
      <link>http://localhost:1313/posts/lora/summary/</link>
      <pubDate>Wed, 22 Oct 2025 11:26:01 +0800</pubDate>
      <guid>http://localhost:1313/posts/lora/summary/</guid>
      <description></description>
    </item>
    <item>
      <title>ETHShanghai_Hackathon_2025</title>
      <link>http://localhost:1313/posts/ethshanghai_hackathon_2025/</link>
      <pubDate>Wed, 22 Oct 2025 08:05:45 +0800</pubDate>
      <guid>http://localhost:1313/posts/ethshanghai_hackathon_2025/</guid>
      <description>&lt;h3 id=&#34;概述&#34;&gt;概述&lt;/h3&gt;
&lt;p&gt;最近参加了ETHShanghai的黑客松，还算是有点收获，以下从参赛感受、项目思路等几个方面展开。&lt;/p&gt;
&lt;h3 id=&#34;参赛感受&#34;&gt;参赛感受&lt;/h3&gt;
&lt;p&gt;首先，这次的黑客松并不是在网上看到的那样，三天内从零开发一款产品demo。大多数组是提前完成了项目后，将此作为推广平台来展示的。这就导致我们最终的半成品很难竞争。&lt;/p&gt;
&lt;p&gt;其次，前端和PPT占很大比重。由于时间问题，如何鲜明的表达主题、功能和PPT提出的设想往往是项目成功与否的决定因素，但是这只是一个trick。大多数Web3网站的前端极其优美，这已经成为一个门槛。&lt;/p&gt;
&lt;p&gt;最后，参赛者以学生和有一定互联网工作经历的人（有很强的人！）为主。我觉得虽然目前Web3的开发者只有20000人（Web2则有2000万），但是Web2向Web3迁移确实正在发生。给我的直观感受是许多工作仍需要基于Web2时代的技术栈，链与数据库概念相似，是一种分享型数据库。&lt;/p&gt;
&lt;h3 id=&#34;nomos项目思路&#34;&gt;NOMOS项目思路&lt;/h3&gt;
&lt;p&gt;由于某些原因，我们在比赛前两天才确定主题。对标的项目是游民岛和zupass，目的是组织数字游民并尝试构建社区。理想功能有：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;任务发布与接收，含报酬支付&lt;/li&gt;
&lt;li&gt;任务完成度纠纷协商机制，防止发布方和接收方各执一词，此时需要第三方的评鉴&lt;/li&gt;
&lt;li&gt;住宿服务，尝试与线下真实资产链接&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;技术的关键是游民的灵魂NFT铸造和平台通用的信誉评分。问题是在这个项目中，往往是为了使用这个技术而去实现，所以给我的体验并不好。我主要是负责信誉评分模块，参考了Talent Protocol的Build Score和Okapi的Degen Score设计，本质上是使用用户的行为数据：接受任务数、纠纷成功数、活跃度等等，将其量化为指标并于用户的唯一身份绑定。这个似乎不能称作算法，就是加权获取的，有没有更加客观的评分实现方式？比如说能动态规划指标的权重，使不同时间的不同权重不同？&lt;/p&gt;
&lt;p&gt;这次我还尝试负责了后端的设计，使用GIN框架实现。由于是claude辅助设计，后续需要对GIN做一个系统总结。事实上，我只了解到了handler与路由、方法的绑定，虽然够用，但是还是不全面，后续会补齐日志。&lt;/p&gt;
&lt;h3 id=&#34;其他&#34;&gt;其他&lt;/h3&gt;
&lt;p&gt;参与Web3的活动有一段时间了，其中鱼龙混杂。无论是线下交流会还是技术竞赛，会发现开发者和投资者混合，往往需要深入了解才能分别：因为许多人讲述概念很有经验，但可能不会编程。相比之下，我觉得作为有一定经验开发者入行的体验会更好，而且不宜深入。&lt;/p&gt;
&lt;p&gt;一方面，区块链领域的先进技术很难在这些活动中出现。因为活动以宣传、拉投资为主，而真正的研究员的技术晦涩难懂，无论是共识机制还是密码学安全保障，这导致了学术&amp;lt;&amp;ndash;&amp;gt;宣传&amp;lt;&amp;ndash;&amp;gt;技术有些割裂。&lt;/p&gt;
&lt;p&gt;另一方面，我觉得目前Web3还是投资性质占大头，因为链给我的感觉就是类似数据库的概念。对我来说是大公司还是DAO组织没有区别，个人的认知层并没有这么高。链所提倡的去中心化、安全性与数据库的中心化、高效是不同trade off的结果，并不是哪个注定更优的。现阶段，由链实现的全球支付无疑是最吸引人的，但除此之外我就没了解到什么既激动人心又浅显易懂的叙事了。&lt;/p&gt;
&lt;p&gt;即便如此，还是持续有新鲜血液涌入这个行业，这无疑是有益于Web3的发展的，我的同学也十分热衷于为此作贡献。接下来会基于ERC-8004学习和开发，希望借此对Web3加深理解。&lt;/p&gt;</description>
    </item>
    <item>
      <title>记录迁移学习综述</title>
      <link>http://localhost:1313/posts/transferlearning/</link>
      <pubDate>Sun, 28 Sep 2025 20:05:27 +0800</pubDate>
      <guid>http://localhost:1313/posts/transferlearning/</guid>
      <description>&lt;h4 id=&#34;迁移学习主要的实现方式&#34;&gt;迁移学习主要的实现方式&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;使用预训练模型(Using Pre-trained Models)&lt;/li&gt;
&lt;li&gt;使用抽取特征(Feature Extraction)
这里可能需要与第一点区别，我的理解是这里只提取关键层的特征，比如视觉模型的识别边缘、形状、颜色的能力。&lt;/li&gt;
&lt;li&gt;模型微调(Model Fine-tuning)&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;迁移学习的优势&#34;&gt;迁移学习的优势&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;减少训练时间，同时增强原有模型的效果。&lt;/li&gt;
&lt;li&gt;缓解数据缺乏的问题。&lt;/li&gt;
&lt;li&gt;更好的泛化性。在这里迁移学习更像是一种学习范式，它可以让GPT或BERT模型的能力提升。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;迁移学习技术&#34;&gt;迁移学习技术&lt;/h4&gt;
&lt;p&gt;根据源数据与目标数据的关系，可以将迁移学习技术分为以下几类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;归纳迁移学习(Inductive Transfer Learning)
源数据训练的模型与目标数据正相关，这意味着模型只需要继续从目标数据中提取特征即可。&lt;/li&gt;
&lt;li&gt;无监督迁移学习(Unsupervised Transfer Learning)
这常用于目标任务没有可用标记数据的情况。这种方法使模型能够从原任务中学习通用特征，并将这些知识迁移到目标任务中。&lt;/li&gt;
&lt;li&gt;传导迁移学习(Transductive Transfer Learning)&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Virtuals Hackathon AI模块开发记录</title>
      <link>http://localhost:1313/posts/virtualshackathon/</link>
      <pubDate>Sat, 27 Sep 2025 08:59:57 +0800</pubDate>
      <guid>http://localhost:1313/posts/virtualshackathon/</guid>
      <description>&lt;h3 id=&#34;ai任务定义&#34;&gt;AI任务定义&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;核心任务：根据市场状态，决定资金在 &lt;strong&gt;“避险资产（Aave）”&lt;/strong&gt; 和 &lt;strong&gt;“风险/收益资产（Uniswap V3 LP）”&lt;/strong&gt; 之间的分配比例，并为风险资产提供最优的参数。&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
  </channel>
</rss>
