+++
date = '2025-10-23T15:48:08+08:00'
draft = false
title = 'LoRA论文感悟'
+++

这次阅读的是LoRA论文，链接：https://arxiv.org/abs/2106.09685

#### LoRA与Adapter的区别
Adapter是谷歌19年提出的，LoRA是微软21年提出的。前者通过增加可训练模块实现，以Transformer的Encoder为例，原本的输入是经过:

```
Multi-Head Attention --> 残差连接+LayerNorm --> Feed Forward(MLP) --> 残差连接+LayerNorm
```

经过Adapter处理后，两处残差连接发生了变化，主要是前一个层的输出要额外处理：
```
x --> W_1 * x --> ReLU(W1 * x) --> W_2*(ReLU(W1 * x)) --> x + W_2*(ReLU(W1 * x)) --> 输出
```

他的优点是只需要训练W_1和W_2，可以独立添加多个任务的Adapter模块。
但是这会导致推理延迟增加，毕竟矩阵乘法和非线性激活需要多执行几次。它还使网络结构变得更加复杂。参数相关显存整体上是下降了30~100倍，但是激活显存略有增加（5%~10%）。

相比之下，LoRA的关键优势是不插入新层，而是在已有层的权重上做“低秩偏移”，从而做到几乎无延迟。但是不是所有层都可以处理的，主要是Attention的Wq/Wv。

#### LoRA的核心思想
传统权重更新是
```
W = W_0 + ΔW
```
作者假设ΔW存在低秩结构，有用的变化集中在一个低维子空间中，只要训练两个小矩阵就可以了:
```
ΔW = B * A

A ：r * k
B * d * r
r << min(d, k)
```
可以使用奇异值分解理解低秩，目标是为了绕过显示的SVD，参数量从 _O(d*k)_ 下降到 _O(r*(d+k))_

#### 论文中的关键直觉

1. 为什么选择对注意力层操作而不是对MLP层操作

    Transformer的核心计算在多头注意力，其中的矩阵的规模大、直接控制注意力权重的投影方式，因此对任务特定模式及其敏感。

    MLP层的任务主要是非线性特征混合，而不是语义结构建模。微调时，它们的梯度变化相对平滑，加入LoRA的收益小，反而浪费参数预算。

2. 为什么作者选择 _W_q_ 和 _W_v_

    Query决定关注哪里，Value关注取回什么内容，Key影响打分权重，Output做线性映射。LoRA主要是为了重新定义注意力语义空间，而不是将已有的权重加强。

3. 低秩低到多少就有效了？不同秩的LoRA学到的方向是否相似？ΔW与W是否存在相关性？

    - 首先是第一个问题，这里作者尝试了不同的rank，结果如下：
        | r  | WikiSQL | MNLI  |
        | -- | ------- | ----- |
        | 1  | 73.4%   | 91.3% |
        | 4  | 73.7%   | 91.3% |
        | 8  | 73.8%   | 91.6% |
        | 64 | 73.5%   | 91.4% |
        这意味着：模型微调所需的“方向空间”非常小（在 12,000 维权重空间中只需 1~4 维调整）。

    - 第二个问题中，作者比较了不同 rank 的 LoRA 矩阵的子空间相似度。

        设 $A_{r=8}$ 和 $A_{r=64}$ 是 rank 分别为 8 和 64 的 LoRA 参数矩阵。

        对它们做 SVD：

        $$
        A_{r=8} = U_8 \Sigma_8 V_8^\top, \quad A_{r=64} = U_{64} \Sigma_{64} V_{64}^\top
        $$

        取右奇异向量空间 $U_8, U_{64}$，定义归一化相似度：

        $$
        \phi(A_8, A_{64}, i, j) = \frac{\|U_{8,1:i}^\top U_{64,1:j}\|_F^2}{\min(i,j)} \in [0,1]
        $$

        这个指标衡量两个子空间的“重叠程度”：

        - $\phi = 1$：完全相同；
        - $\phi = 0$：完全正交。

        r=8 的前几个奇异方向与 r=64 的主方向几乎完全重叠；后续方向迅速无关；表明 LoRA 学到的主要信息集中在前 1~2 个主方向。
    
    - 最后，作者研究 LoRA 学到的更新矩阵 ΔW 与原权重 W 的关系。

        他将 ΔW 做SVD，然后把原模型权重在 ΔW 的方向上投影，然后比较 F范数（直觉上F范数越大说明能量越大？），结果如下：

        | ITEM            | $(\|U^\top W V\|_F)$ | $(\|W\|_F)$ | $(\|\Delta W\|_F)$ |
        |-----------------|----------------------|--------------|---------------------|
        | ΔWq             | 0.32                 | 61.95        | 6.91                |
        | Wq(self)        | 21.67                | 61.95        | 6.91                |
        | random matrix   | 0.02                 | -            | -                   |

        如果 ΔW 完全独立于 W ， $(\|U^\top W V\|_F)$ 应接近 0；实测是 0.32 ≫ 0.02（随机噪声），说明它们方向相关；但 0.32 远小于 21.67（W 自身主方向），说明 ΔW 专注于 W 中“非主方向”。换句话说，ΔW 并非重学主特征，而是放大 W 中那些“被忽略的次要方向”。它不是推翻旧知识，而是增强了模型已有但被轻视的特征方向。
